#! /bin/bash

set -e -x

sudo -E -u hdfs hadoop fs -mkdir -p /user/hive/warehouse
sudo -E -u hdfs hadoop fs -chmod a+w /user/hive/warehouse
sudo -E -u hdfs hadoop fs -chmod a+w /tmp

cd /home/hive
sudo -E -u hive nohup ${HIVE_HOME}/bin/hiveserver2 > /home/hive/hive.log & sleep 180

cd /home/jenkins
# Download dataset and upload it to HDFS
wget https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/AirlinesTest.csv.zip
unzip AirlinesTest.csv.zip
rm AirlinesTest.csv.zip
sed -i 's/\"//g' AirlinesTest.csv

sudo -E -u hive hadoop fs -put -f ./AirlinesTest.csv /tmp/AirlinesTest.csv

# Execute all hive-scripts
cd /opt/hive-scripts
KRB_SUFFIX=
if [ "${KRB_ENABLED}" = "true" ]; then
    KRB_SUFFIX=';principal=hive/localhost@H2O.AI'
fi
for f in $(ls); do
    sudo -E -u hive ${HIVE_HOME}/bin/beeline -u "jdbc:hive2://localhost:10000/${KRB_SUFFIX}" -f ${f}
done

sudo -E -u hive hadoop fs -put -f /home/jenkins/AirlinesTest.csv /tmp/AirlinesTest.csv
rm /home/jenkins/AirlinesTest.csv
